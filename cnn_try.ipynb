{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIzICjaph_Wy"
      },
      "source": [
        "<a align=\"center\" href=\"https://ultralytics.com/hub\" target=\"_blank\">\n",
        "<img width=\"1024\", src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\"></a>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "[‰∏≠Êñá](https://docs.ultralytics.com/zh/hub/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/hub/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/hub/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/hub/) | [Deutsch](https://docs.ultralytics.com/de/hub/) | [Fran√ßais](https://docs.ultralytics.com/fr/hub/) | [Espa√±ol](https://docs.ultralytics.com/es/hub/) | [Portugu√™s](https://docs.ultralytics.com/pt/hub/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/hub/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/hub/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/hub/)\n",
        "\n",
        "  <a href=\"https://github.com/ultralytics/hub/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/hub/actions/workflows/ci.yml/badge.svg\" alt=\"CI CPU\"></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/hub/blob/main/hub.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
        "  <a href=\"https://community.ultralytics.com\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n",
        "  <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n",
        "\n",
        "Welcome to the [Ultralytics](https://ultralytics.com/) HUB notebook!\n",
        "\n",
        "This notebook allows you to train Ultralytics [YOLO](https://github.com/ultralytics/ultralytics) üöÄ models using [HUB](https://hub.ultralytics.com/). Please browse the HUB <a href=\"https://docs.ultralytics.com/hub/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/hub/issues/new/choose\">GitHub</a> for support, and join our <a href=\"https://ultralytics.com/discord\">Discord</a> community for questions and discussions!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRQ2ow94MiOv"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyDnXd-n4c7Y",
        "outputId": "bb836f79-9154-47c6-abc7-f1acd912ff3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.99 üöÄ Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 39.6/112.6 GB disk)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ9BwaAqxAm4"
      },
      "source": [
        "# Start\n",
        "\n",
        "‚ö° Login with your API key, load your YOLO üöÄ model, and start training in 3 lines of code!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"/root/.cache/kagglehub/datasets\"):\n",
        "    for dir_name in dirs:\n",
        "        print(os.path.join(root, dir_name))"
      ],
      "metadata": {
        "id": "V7rv_F24IGmo",
        "outputId": "497f433a-2ccc-4428-f20f-54cd3eb36a3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/kagglehub/datasets/iraqyomar\n",
            "/root/.cache/kagglehub/datasets/iraqyomar/khatt-arabic-hand-written-lines\n",
            "/root/.cache/kagglehub/datasets/iraqyomar/khatt-arabic-hand-written-lines/versions\n",
            "/root/.cache/kagglehub/datasets/iraqyomar/khatt-arabic-hand-written-lines/versions/1\n",
            "/root/.cache/kagglehub/datasets/iraqyomar/khatt-arabic-hand-written-lines/versions/1/images\n",
            "/root/.cache/kagglehub/datasets/iraqyomar/khatt-arabic-hand-written-lines/versions/1/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"iraqyomar/khatt-arabic-hand-written-lines\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "93uEfWFcIEjw",
        "outputId": "ce98ac51-af19-4414-e02d-18bdde40aca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/iraqyomar/khatt-arabic-hand-written-lines?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 295M/295M [00:02<00:00, 104MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/iraqyomar/khatt-arabic-hand-written-lines/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSlZaJ9Iw_iZ",
        "outputId": "44ea606c-7e44-40a4-8776-d571d27633e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train loss: 0.0019\n",
            "Sample decode: ['', '', '']\n",
            "[Epoch 2] Train loss: 0.0011\n",
            "Sample decode: ['', '', '']\n",
            "[Epoch 3] Train loss: 0.0009\n",
            "Sample decode: ['ŸÜÿ±', 'ŸÜÿ±', 'ŸÜÿ±']\n",
            "[Epoch 4] Train loss: 0.0001\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 5] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 6] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 7] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 8] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 9] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 10] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 11] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 12] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 13] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 14] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n",
            "[Epoch 15] Train loss: 0.0000\n",
            "Sample decode: ['ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±', 'ŸÜÿ≥ÿ±']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === paths ===\n",
        "IMG_DIR = \"/root/.cache/kagglehub/datasets/iraqyomar/khatt-arabic-hand-written-lines/versions/1/images\"\n",
        "LBL_DIR = \"/root/.cache/kagglehub/datasets/iraqyomar/khatt-arabic-hand-written-lines/versions/1/labels\"\n",
        "\n",
        "# === hyperparams ===\n",
        "IMG_W, IMG_H = 128, 32\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 40\n",
        "VALID_SPLIT = 0.1\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# === utility: prefix matching for labels ===\n",
        "def build_prefix_map(label_dir):\n",
        "    prefix_map = {}\n",
        "    for fname in sorted(os.listdir(label_dir)):\n",
        "        if not fname.lower().endswith(\".txt\"):\n",
        "            continue\n",
        "        prefix = fname.split(\"_\", 1)[0]\n",
        "        if prefix not in prefix_map:\n",
        "            prefix_map[prefix] = os.path.join(label_dir, fname)\n",
        "    return prefix_map\n",
        "\n",
        "# === dataset ===\n",
        "class KHATTDataset(Dataset):\n",
        "    def __init__(self, img_dir, lbl_dir, char_to_idx, max_samples=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.lbl_dir = lbl_dir\n",
        "        self.char_to_idx = char_to_idx\n",
        "        self.prefix_map = build_prefix_map(lbl_dir)\n",
        "        self.img_files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
        "        if max_samples:\n",
        "            self.img_files = self.img_files[:max_samples]\n",
        "        self.samples = []\n",
        "        for fname in self.img_files:\n",
        "            prefix = fname.split(\"_\", 1)[0]\n",
        "            lbl_path = self.prefix_map.get(prefix)\n",
        "            if lbl_path is None:\n",
        "                continue\n",
        "            img_path = os.path.join(img_dir, fname)\n",
        "            label = None\n",
        "            for enc in [\"windows-1256\", \"utf-8\", \"latin1\"]:\n",
        "                try:\n",
        "                    with open(lbl_path, \"r\", encoding=enc) as f:\n",
        "                        label = f.read().strip()\n",
        "                    if label:\n",
        "                        break\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if not label:\n",
        "                continue\n",
        "            self.samples.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (IMG_W, IMG_H))\n",
        "        img = img.astype(\"float32\") / 255.0  # normalize\n",
        "        img = torch.from_numpy(img).unsqueeze(0)  # (1, H, W)\n",
        "\n",
        "        # encode label (no blank; CTC uses 0 as blank implicitly)\n",
        "        label_seq = [self.char_to_idx[c] for c in label if c in self.char_to_idx]\n",
        "        label_tensor = torch.tensor(label_seq, dtype=torch.long)\n",
        "        return img, label_tensor\n",
        "\n",
        "# === character mapping ===\n",
        "def build_char_map(dataset):\n",
        "    all_text = \"\".join([lbl for _, lbl in dataset.samples])\n",
        "    chars = sorted(set(all_text))\n",
        "    # map chars -> 1..N, reserving 0 for blank\n",
        "    char_to_idx = {c: i + 1 for i, c in enumerate(chars)}\n",
        "    idx_to_char = {i + 1: c for i, c in enumerate(chars)}\n",
        "    return char_to_idx, idx_to_char\n",
        "\n",
        "# === collate for variable-length labels ===\n",
        "def collate_fn(batch):\n",
        "    imgs, labels = zip(*batch)\n",
        "    imgs = torch.stack(imgs)  # (B,1,H,W)\n",
        "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
        "    labels_concat = torch.cat(labels)  # needed by CTCLoss\n",
        "    return imgs, labels_concat, label_lengths\n",
        "\n",
        "# === model: CNN -> BiLSTM -> linear (vocab+1) ===\n",
        "class OCRModel(nn.Module):\n",
        "    def __init__(self, num_chars):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),  # (B,32,H,W)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),              # (B,32,H/2,W/2)\n",
        "            nn.Conv2d(32, 64, 3, padding=1), # (B,64,H/2,W/2)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),              # (B,64,H/4,W/4)\n",
        "        )\n",
        "        # after two poolings: H' = IMG_H//4, W' = IMG_W//4\n",
        "        self.rnn_input_size = (IMG_W // 4) * 64  # treating height as time\n",
        "        self.bi_lstm = nn.LSTM(self.rnn_input_size, 128, bidirectional=True, batch_first=True)\n",
        "        self.classifier = nn.Linear(128 * 2, num_chars + 1)  # +1 for CTC blank\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,1,H,W)\n",
        "        x = self.cnn(x)  # (B,64,H/4,W/4)\n",
        "        # reshape: time dimension = height after conv (IMG_H//4)\n",
        "        b, c, h, w = x.size()\n",
        "        # collapse width*channels as features per time-step\n",
        "        x = x.permute(0, 2, 1, 3)  # (B, H', C, W')\n",
        "        x = x.contiguous().view(b, h, c * w)  # (B, time, feat)\n",
        "        x, _ = self.bi_lstm(x)  # (B, time, 256)\n",
        "        x = self.classifier(x)  # (B, time, num_chars+1)\n",
        "        x = x.log_softmax(2)    # for CTCLoss expects log-probs\n",
        "        return x  # (B, time, vocab+1)\n",
        "\n",
        "# === training setup ===\n",
        "# temporary dataset to build char maps\n",
        "temp_ds = KHATTDataset(IMG_DIR, LBL_DIR, char_to_idx={})  # will ignore char_to_idx here\n",
        "char_to_idx, idx_to_char = build_char_map(temp_ds)\n",
        "dataset = KHATTDataset(IMG_DIR, LBL_DIR, char_to_idx)\n",
        "# split\n",
        "n = len(dataset)\n",
        "indices = list(range(n))\n",
        "random.shuffle(indices)\n",
        "split = int(n * (1 - VALID_SPLIT))\n",
        "train_idx, val_idx = indices[:split], indices[split:]\n",
        "train_ds = torch.utils.data.Subset(dataset, train_idx)\n",
        "val_ds = torch.utils.data.Subset(dataset, val_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "model = OCRModel(num_chars=len(char_to_idx)).to(DEVICE)\n",
        "ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# time steps (input_length) is constant: IMG_H//4\n",
        "input_length_val = torch.full((BATCH_SIZE,), IMG_H // 4, dtype=torch.long)\n",
        "\n",
        "def greedy_decode(log_probs, idx_to_char):\n",
        "    # log_probs: (B, time, vocab+1)\n",
        "    preds = torch.argmax(log_probs, dim=2)  # (B, time)\n",
        "    results = []\n",
        "    for seq in preds:\n",
        "        prev = -1\n",
        "        chars = []\n",
        "        for idx in seq.cpu().numpy():\n",
        "            if idx != prev and idx != 0:  # skip blanks and repeats\n",
        "                chars.append(idx_to_char.get(idx, \"\"))\n",
        "            prev = idx\n",
        "        results.append(\"\".join(chars))\n",
        "    return results\n",
        "\n",
        "# === training loop ===\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for imgs, labels_concat, label_lengths in train_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        # compute predicted log-probs\n",
        "        log_probs = model(imgs)  # (B, time, vocab+1)\n",
        "        batch_size, time_steps, _ = log_probs.size()\n",
        "        log_probs = log_probs.permute(1, 0, 2)  # CTCLoss expects (T, N, C)\n",
        "\n",
        "        # prepare inputs for CTCLoss\n",
        "        input_lengths = torch.full((batch_size,), time_steps, dtype=torch.long).to(DEVICE)\n",
        "        target_lengths = label_lengths.to(DEVICE)\n",
        "        labels_concat = labels_concat.to(DEVICE)\n",
        "\n",
        "        loss = ctc_loss(log_probs, labels_concat, input_lengths, target_lengths)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_size\n",
        "\n",
        "    avg = total_loss / len(train_ds)\n",
        "    print(f\"[Epoch {epoch}] Train loss: {avg:.4f}\")\n",
        "\n",
        "    # validation quick check (no gradient)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels_concat, label_lengths in val_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            log_probs = model(imgs)  # (B, time, vocab+1)\n",
        "            decoded = greedy_decode(log_probs, idx_to_char)\n",
        "            print(\"Sample decode:\", decoded[:3])\n",
        "            break  # just one batch preview\n",
        "\n",
        "# === save model ===\n",
        "torch.save({\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"char_to_idx\": char_to_idx,\n",
        "    \"idx_to_char\": idx_to_char\n",
        "}, \"ocr_ctc_pytorch.pt\")\n",
        "print(\"Saved PyTorch model to ocr_ctc_pytorch.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# === Load model checkpoint ===\n",
        "checkpoint = torch.load(\"/content/ocr_ctc_pytorch.pt\", map_location='cpu')\n",
        "char_to_idx = checkpoint[\"char_to_idx\"]\n",
        "idx_to_char = checkpoint[\"idx_to_char\"]\n",
        "num_chars = len(char_to_idx)\n",
        "\n",
        "# === Match model architecture ===\n",
        "class OCRModel(torch.nn.Module):\n",
        "    def __init__(self, num_chars):\n",
        "        super().__init__()\n",
        "        self.cnn = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, 3, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(2, 2),\n",
        "            torch.nn.Conv2d(32, 64, 3, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        self.bi_lstm = torch.nn.LSTM((128 // 4) * 64, 128, bidirectional=True, batch_first=True)\n",
        "        self.classifier = torch.nn.Linear(128 * 2, num_chars + 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)  # [B, 64, H/4, W/4]\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(b, h, c * w)\n",
        "        x, _ = self.bi_lstm(x)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x, dim=2)\n",
        "\n",
        "# === Initialize model ===\n",
        "model = OCRModel(num_chars=num_chars)\n",
        "model.load_state_dict(checkpoint[\"model_state\"])\n",
        "model.eval()\n",
        "\n",
        "# === Preprocessing ===\n",
        "IMG_W, IMG_H = 128, 32\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (IMG_W, IMG_H))\n",
        "    img = img.astype(\"float32\") / 255.0\n",
        "    img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n",
        "    return img\n",
        "\n",
        "# === Greedy decode ===\n",
        "def greedy_decode(log_probs, idx_to_char):\n",
        "    preds = torch.argmax(log_probs, dim=2)  # [B, T]\n",
        "    results = []\n",
        "    for seq in preds:\n",
        "        prev = -1\n",
        "        chars = []\n",
        "        for idx in seq.numpy():\n",
        "            if idx != prev and idx != 0:\n",
        "                chars.append(idx_to_char.get(idx, \"\"))\n",
        "            prev = idx\n",
        "        results.append(\"\".join(chars))\n",
        "    return results\n",
        "\n",
        "# === Inference ===\n",
        "test_image_path = \"/content/AHTD3A0083_Para3_4.jpg\"  # üîÅ Change this path\n",
        "img_tensor = preprocess_image(test_image_path)\n",
        "with torch.no_grad():\n",
        "    log_probs = model(img_tensor)\n",
        "    decoded = greedy_decode(log_probs, idx_to_char)\n",
        "\n",
        "print(\"üìù Predicted Text:\", decoded[0])\n"
      ],
      "metadata": {
        "id": "q7tPype5JPZ0",
        "outputId": "009f2445-6881-4154-e8d1-ff9b6f7aa181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Predicted Text: ŸÜÿ≥ÿ±\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Ultralytics HUB",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}